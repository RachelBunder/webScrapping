{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "* Install scrapy and jupyter\n",
    "* First we are going to explore, using:\n",
    "\n",
    "`scrapy shell https://en.wikipedia.org/wiki/Sydney`\n",
    "\n",
    "\n",
    "Or your wiki page of choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why wiki?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does wiki's code look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the title:\n",
    "\n",
    "Try:\n",
    "\n",
    "`response.xpath('//title')`\n",
    "\n",
    "`response.xpath('//title').extract()`\n",
    "\n",
    "`response.xpath('//title/text()').extract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is contained in the body?\n",
    "\n",
    "```python\n",
    "response.xpath('//body')\n",
    "response.xpath('//body/div')\n",
    "\n",
    "response.xpath('//body/div/div')\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: getting the table of contents\n",
    "\n",
    "```python\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div')\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/div[@id = \"toc]\"')\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/div[@id = \"toc\"]/ul/li')\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/div[@id = \"toc\"]/ul/li/a/span[@class = \"toctext\"]')\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/div[@id = \"toc\"]/ul/li/a/span[@class = \"toctext\"]/text()').extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our original problem: how do we get the first link from the wiki page\n",
    "\n",
    "```python\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/text()').extract()\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a')[0]\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a')[0]\n",
    "\n",
    "response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a/@href').extract()[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider\n",
    "\n",
    "So we now know where the first link in a wiki page is - we want to follow it. To do we need a spider - scrappy can do this for us\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exit out of the shell\n",
    "* Create a new project\n",
    "\n",
    "`scrapy startproject wiki`\n",
    "\n",
    "* This creates the basic structure for a spider\n",
    "* We need to create a new file for our spider in /wiki/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"wiki\"\n",
    "    allowed_domains = [\"https://en.wikipedia.org\"]\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Sydney\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        print(response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a/@href').extract()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run using:\n",
    "\n",
    "`scrapy crawl wiki`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting crazy errors like:\n",
    "    \n",
    "```shell\n",
    "2016-07-21 11:24:53 [scrapy] INFO: Scrapy 1.0.3 started (bot: wiki)\n",
    "2016-07-21 11:24:53 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-07-21 11:24:53 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'wiki.spiders', 'SPIDER_MODULES': ['wiki.spiders'], 'BOT_NAME': 'wiki'}\n",
    "2016-07-21 11:24:53 [py.warnings] WARNING: :0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n",
    "\n",
    "2016-07-21 11:24:53 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
    "2016-07-21 11:24:53 [boto] DEBUG: Retrieving credentials from metadata server.\n",
    "2016-07-21 11:24:54 [boto] ERROR: Caught exception reading instance data\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/boto/utils.py\", line 214, in retry_url\n",
    "    r = opener.open(req)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/urllib2.py\", line 431, in open\n",
    "    response = self._open(req, data)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/urllib2.py\", line 449, in _open\n",
    "    '_open', req)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/urllib2.py\", line 409, in _call_chain\n",
    "    result = func(*args)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/urllib2.py\", line 1227, in http_open\n",
    "    return self.do_open(httplib.HTTPConnection, req)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/urllib2.py\", line 1197, in do_open\n",
    "    raise URLError(err)\n",
    "URLError: <urlopen error timed out>\n",
    "2016-07-21 11:24:54 [boto] ERROR: Unable to read instance data, giving up\n",
    "2016-07-21 11:24:54 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
    "2016-07-21 11:24:54 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
    "2016-07-21 11:24:54 [scrapy] INFO: Enabled item pipelines: \n",
    "2016-07-21 11:24:54 [scrapy] INFO: Spider opened\n",
    "2016-07-21 11:24:54 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2016-07-21 11:24:54 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "Error during info_callback\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 421, in dataReceived\n",
    "    self._write(bytes)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 569, in _write\n",
    "    sent = self._tlsConnection.send(toSend)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1270, in send\n",
    "    result = _lib.SSL_write(self._ssl, buf, len(buf))\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 926, in wrapper\n",
    "    callback(Connection._reverse_mapping[ssl], where, return_code)\n",
    "--- <exception caught here> ---\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1154, in infoCallback\n",
    "    return wrapped(connection, where, ret)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1256, in _identityVerifyingInfoCallback\n",
    "    transport = connection.get_app_data()\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1589, in get_app_data\n",
    "    return self._app_data\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1148, in __getattr__\n",
    "    return getattr(self._socket, name)\n",
    "exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'\n",
    "\n",
    "2016-07-21 11:24:55 [twisted] CRITICAL: Error during info_callback\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 421, in dataReceived\n",
    "    self._write(bytes)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 569, in _write\n",
    "    sent = self._tlsConnection.send(toSend)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1270, in send\n",
    "    result = _lib.SSL_write(self._ssl, buf, len(buf))\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 926, in wrapper\n",
    "    callback(Connection._reverse_mapping[ssl], where, return_code)\n",
    "--- <exception caught here> ---\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1154, in infoCallback\n",
    "    return wrapped(connection, where, ret)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1256, in _identityVerifyingInfoCallback\n",
    "    transport = connection.get_app_data()\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1589, in get_app_data\n",
    "    return self._app_data\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1148, in __getattr__\n",
    "    return getattr(self._socket, name)\n",
    "exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'\n",
    "\n",
    "From callback <function infoCallback at 0x10598ab18>:\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 926, in wrapper\n",
    "    callback(Connection._reverse_mapping[ssl], where, return_code)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1158, in infoCallback\n",
    "    connection.get_app_data().failVerification(f)\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1589, in get_app_data\n",
    "    return self._app_data\n",
    "  File \"/Users/rachel/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1148, in __getattr__\n",
    "    return getattr(self._socket, name)\n",
    "AttributeError: 'NoneType' object has no attribute '_app_data'\n",
    "2016-07-21 11:24:55 [scrapy] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Sydney> (referer: None)\n",
    "```\n",
    "\n",
    "This is a problem with verifying wiki's security certificate. Try installing the service_identify package:\n",
    "\n",
    "```shell\n",
    "conda install service_identity\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```shell\n",
    "pip install service_identity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"wiki\"\n",
    "    allowed_domains = [\"en.wikipedia.org\"]\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Sydney\"]\n",
    "\n",
    "    base_url = \"https://en.wikipedia.org\"\n",
    "    def parse(self, response):\n",
    "        \n",
    "        next_link = response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a/@href').extract()[0]\n",
    "\n",
    "        print next_link\n",
    "\n",
    "        yield scrapy.Request(self.base_url + next_link, callback = self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the links\n",
    "\n",
    "* Open items.py\n",
    "* Here we need to define a class that contains the information we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    link = scrapy.Field()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from wiki.items import WikiItem\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"wiki\"\n",
    "    allowed_domains = [\"en.wikipedia.org\"]\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Sydney\"]\n",
    "\n",
    "    base_url = \"https://en.wikipedia.org\"\n",
    "    def parse(self, response):\n",
    "        \n",
    "        link = WikiItem()\n",
    "\n",
    "        link[\"link\"] = response.xpath('//body/div/div[@id = \"bodyContent\"]/div/p/a/@href').extract()[0]\n",
    "\n",
    "\n",
    "        yield scrapy.Request(self.base_url + link[\"link\"], callback = self.parse)\n",
    "\n",
    "        yield link\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
